{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "from scipy import sparse\n",
    "from scipy.optimize import nnls\n",
    "from scipy.stats import zscore, percentileofscore\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sc.settings.verbosity = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_data='/private/groups/russelllab/jodie/wolbachia_induced_DE/scanpy_clustering/scanpy_objects/bulk_adata.h5ad'\n",
    "ref_path='/private/groups/russelllab/jodie/wolbachia_induced_DE/scanpy_clustering/data/atlas/fca_subset.h5ad'\n",
    "output_dir='/private/groups/russelllab/jodie/wolbachia_induced_DE/wolbachia_induced_differentiation/scripts/celltype_clustering/claude/deconvolution/output'\n",
    "annotation_key='annotation'\n",
    "n_markers=100\n",
    "n_bootstraps=1\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up output directory and plotting parameters.\"\"\"\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plots_dir = os.path.join(output_dir, 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Set up log file\n",
    "log_file = os.path.join(output_dir, 'deconvolution_log.txt')\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Set scanpy settings\n",
    "sc.settings.figdir = plots_dir\n",
    "sc.settings.set_figure_params(dpi=300, frameon=False, figsize=(10, 8), facecolor='white')\n",
    "\n",
    "# Define custom color palette for cell types\n",
    "custom_palette = sns.color_palette(\"husl\", 100)  # Generate a large color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def identify_cell_type_markers(adata, groupby, n_markers=100):\n",
    "    \"\"\"\n",
    "    Identify marker genes for each cell type using a modified TF-IDF approach.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object\n",
    "        groupby: Column name in adata.obs for cell type annotations\n",
    "        n_markers: Number of marker genes to select per cell type\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping cell types to marker genes with weights\n",
    "    \"\"\"\n",
    "    logger.info(f\"Identifying marker genes for each cell type using {groupby}...\")\n",
    "    \n",
    "    # Run standard Scanpy differential expression to get initial markers\n",
    "    sc.tl.rank_genes_groups(adata, groupby, method='wilcoxon')\n",
    "    \n",
    "    # Get the list of all cell types\n",
    "    cell_types = adata.obs[groupby].cat.categories.tolist()\n",
    "    logger.info(f\"Found {len(cell_types)} cell types\")\n",
    "    \n",
    "    # Create a \"document\" for each cell type consisting of gene expression\n",
    "    # Convert adata to dense format for cell type aggregation if needed\n",
    "    if sparse.issparse(adata.X):\n",
    "        adata_dense = adata.X.toarray()\n",
    "    else:\n",
    "        adata_dense = adata.X\n",
    "    \n",
    "    # Create cell type expression profiles (mean expression per cell type)\n",
    "    cell_type_profiles = {}\n",
    "    gene_names = adata.var_names.tolist()\n",
    "    \n",
    "    for cell_type in cell_types:\n",
    "        mask = adata.obs[groupby] == cell_type\n",
    "        # Skip if no cells for this type\n",
    "        if not np.any(mask):\n",
    "            logger.warning(f\"No cells found for {cell_type}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate mean expression for this cell type\n",
    "        cell_type_profiles[cell_type] = np.mean(adata_dense[mask, :], axis=0)\n",
    "    \n",
    "    # Create term-frequency matrix (cell types × genes)\n",
    "    tf_matrix = np.zeros((len(cell_type_profiles), len(gene_names)))\n",
    "    for i, cell_type in enumerate(cell_type_profiles):\n",
    "        tf_matrix[i, :] = cell_type_profiles[cell_type]\n",
    "    \n",
    "    # Apply TF-IDF transformation\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_matrix = tfidf.fit_transform(tf_matrix)\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if sparse.issparse(tfidf_matrix):\n",
    "        tfidf_matrix = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Extract top markers for each cell type\n",
    "    markers_dict = {}\n",
    "    for i, cell_type in enumerate(cell_type_profiles):\n",
    "        # Get gene scores for this cell type\n",
    "        gene_scores = tfidf_matrix[i, :]\n",
    "        \n",
    "        # Sort genes by TF-IDF score\n",
    "        sorted_indices = np.argsort(-gene_scores)  # Descending order\n",
    "        \n",
    "        # Take top n_markers genes\n",
    "        top_indices = sorted_indices[:n_markers]\n",
    "        \n",
    "        # Store gene names and scores\n",
    "        markers_dict[cell_type] = {\n",
    "            'genes': [gene_names[idx] for idx in top_indices],\n",
    "            'scores': [gene_scores[idx] for idx in top_indices]\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Identified {n_markers} marker genes for each of {len(markers_dict)} cell types\")\n",
    "    \n",
    "    # Plot heatmap of top 10 marker genes per cell type\n",
    "    plot_marker_heatmap(adata, markers_dict, groupby)\n",
    "    \n",
    "    return markers_dict\n",
    "\n",
    "\n",
    "def plot_marker_heatmap(adata, markers_dict, groupby, n_top=10):\n",
    "    \"\"\"\n",
    "    Create a heatmap of top marker genes per cell type.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object\n",
    "        markers_dict: Dictionary of marker genes per cell type\n",
    "        groupby: Column name for cell type annotations\n",
    "        n_top: Number of top genes to include per cell type\n",
    "    \"\"\"\n",
    "    # Collect top n_top genes per cell type\n",
    "    all_top_genes = []\n",
    "    for cell_type in markers_dict:\n",
    "        top_genes = markers_dict[cell_type]['genes'][:n_top]\n",
    "        all_top_genes.extend(top_genes)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_top_genes = []\n",
    "    for gene in all_top_genes:\n",
    "        if gene not in unique_top_genes:\n",
    "            unique_top_genes.append(gene)\n",
    "    \n",
    "    # Create AnnData object with just these genes\n",
    "    if len(unique_top_genes) > 0:\n",
    "        adata_markers = adata[:, unique_top_genes].copy()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sc.pl.heatmap(\n",
    "            adata_markers, var_names=unique_top_genes, \n",
    "            groupby=groupby, \n",
    "            standard_scale='var',  # Scale by gene\n",
    "            cmap='viridis',\n",
    "            swap_axes=True,\n",
    "            show_gene_labels=True,\n",
    "            figsize=(14, 10),\n",
    "            dendrogram=True,\n",
    "            save=\"_top_markers.pdf\"\n",
    "        )\n",
    "        logger.info(\"Created marker gene heatmap\")\n",
    "    else:\n",
    "        logger.warning(\"No marker genes identified for heatmap\")\n",
    "\n",
    "\n",
    "def create_signature_matrix(ref_adata, markers_dict, annotation_key, shared_genes):\n",
    "    \"\"\"\n",
    "    Create a signature matrix from reference data.\n",
    "    \n",
    "    Args:\n",
    "        ref_adata: Reference AnnData object\n",
    "        markers_dict: Dictionary of marker genes per cell type\n",
    "        annotation_key: Column name for cell type annotations\n",
    "        shared_genes: List of genes shared between bulk and reference\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Signature matrix with genes as rows and cell types as columns\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating cell type signature matrix...\")\n",
    "    \n",
    "    # Filter reference data to include only shared genes\n",
    "    ref_subset = ref_adata[:, shared_genes].copy()\n",
    "    \n",
    "    # Get expression for each cell type\n",
    "    cell_types = list(markers_dict.keys())\n",
    "    \n",
    "    # Create empty signature matrix\n",
    "    signature_matrix = pd.DataFrame(index=shared_genes, columns=cell_types)\n",
    "    \n",
    "    # Fill signature matrix with average expression values\n",
    "    for cell_type in cell_types:\n",
    "        # Get cells of this type\n",
    "        cells = ref_subset[ref_subset.obs[annotation_key] == cell_type]\n",
    "        \n",
    "        if cells.shape[0] == 0:\n",
    "            logger.warning(f\"No cells found for {cell_type}, using zeros\")\n",
    "            signature_matrix[cell_type] = 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate mean expression\n",
    "        if sparse.issparse(cells.X):\n",
    "            mean_expr = cells.X.mean(axis=0).A1\n",
    "        else:\n",
    "            mean_expr = cells.X.mean(axis=0)\n",
    "        \n",
    "        # Add to signature matrix\n",
    "        signature_matrix[cell_type] = mean_expr\n",
    "    \n",
    "    # Apply marker gene weighting\n",
    "    for cell_type in cell_types:\n",
    "        if cell_type not in markers_dict:\n",
    "            continue\n",
    "            \n",
    "        # Get marker genes for this cell type\n",
    "        marker_genes = markers_dict[cell_type]['genes']\n",
    "        marker_scores = markers_dict[cell_type]['scores']\n",
    "        \n",
    "        # Only use marker genes that are in shared genes\n",
    "        valid_markers = []\n",
    "        valid_scores = []\n",
    "        for gene, score in zip(marker_genes, marker_scores):\n",
    "            if gene in shared_genes:\n",
    "                valid_markers.append(gene)\n",
    "                valid_scores.append(score)\n",
    "        \n",
    "        # Apply weight to marker genes\n",
    "        for gene, score in zip(valid_markers, valid_scores):\n",
    "            # Emphasize this gene for this cell type by multiplying by score\n",
    "            signature_matrix.at[gene, cell_type] *= (1 + score)\n",
    "    \n",
    "    # Normalize signature matrix (each cell type column sums to 1)\n",
    "    signature_matrix = signature_matrix.apply(lambda x: x / x.sum() if x.sum() > 0 else x, axis=0)\n",
    "    \n",
    "    logger.info(f\"Created signature matrix with {signature_matrix.shape[0]} genes and {signature_matrix.shape[1]} cell types\")\n",
    "    \n",
    "    return signature_matrix\n",
    "\n",
    "\n",
    "def deconvolve_samples(bulk_adata, signature_matrix, shared_genes):\n",
    "    \"\"\"\n",
    "    Deconvolve bulk samples into cell type proportions using signature matrix.\n",
    "    \n",
    "    Args:\n",
    "        bulk_adata: Bulk RNA-seq AnnData\n",
    "        signature_matrix: Signature matrix DataFrame (genes × cell types)\n",
    "        shared_genes: List of genes shared between bulk and reference\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with deconvolution results (samples × cell types)\n",
    "    \"\"\"\n",
    "    logger.info(\"Deconvolving bulk samples into cell type proportions...\")\n",
    "    \n",
    "    # Filter bulk data to include only shared genes\n",
    "    bulk_subset = bulk_adata[:, shared_genes].copy()\n",
    "    \n",
    "    # Prepare output DataFrame\n",
    "    results = pd.DataFrame(index=bulk_subset.obs_names, columns=signature_matrix.columns)\n",
    "    \n",
    "    # Get signature matrix as numpy array (genes × cell types)\n",
    "    S = signature_matrix.values\n",
    "    \n",
    "    # For each bulk sample\n",
    "    for i, sample_id in enumerate(bulk_subset.obs_names):\n",
    "        # Get expression vector for this sample\n",
    "        if sparse.issparse(bulk_subset.X):\n",
    "            b = bulk_subset.X[i].toarray().flatten()\n",
    "        else:\n",
    "            b = bulk_subset.X[i]\n",
    "        \n",
    "        # Solve non-negative least squares problem: min ||Sx - b||^2, s.t. x >= 0\n",
    "        try:\n",
    "            proportions, residual = nnls(S, b)\n",
    "            \n",
    "            # Normalize proportions to sum to 1\n",
    "            if np.sum(proportions) > 0:\n",
    "                proportions = proportions / np.sum(proportions)\n",
    "            \n",
    "            # Store results\n",
    "            results.loc[sample_id] = proportions\n",
    "            \n",
    "            # Log progress for every 10th sample\n",
    "            if (i + 1) % 10 == 0 or i == 0 or i == len(bulk_subset.obs_names) - 1:\n",
    "                logger.info(f\"Deconvolved {i+1}/{len(bulk_subset.obs_names)} bulk samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deconvolving sample {sample_id}: {e}\")\n",
    "            results.loc[sample_id] = np.nan\n",
    "    \n",
    "    logger.info(\"Deconvolution completed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def bootstrap_confidence_intervals(bulk_adata, signature_matrix, shared_genes, n_bootstrap=500):\n",
    "    \"\"\"\n",
    "    Calculate confidence intervals for deconvolution results using bootstrapping.\n",
    "    \n",
    "    Args:\n",
    "        bulk_adata: Bulk RNA-seq AnnData\n",
    "        signature_matrix: Signature matrix DataFrame\n",
    "        shared_genes: List of shared genes\n",
    "        n_bootstrap: Number of bootstrap iterations\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Deconvolution results, lower bound, upper bound) DataFrames\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating confidence intervals using {n_bootstrap} bootstrap iterations...\")\n",
    "    \n",
    "    # Filter bulk data to include only shared genes\n",
    "    bulk_subset = bulk_adata[:, shared_genes].copy()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_results = []\n",
    "    \n",
    "    # Original deconvolution results\n",
    "    original_results = deconvolve_samples(bulk_subset, signature_matrix, shared_genes)\n",
    "    all_results.append(original_results)\n",
    "    \n",
    "    # Bootstrap iterations\n",
    "    for i in range(n_bootstrap):\n",
    "        # Resample genes with replacement\n",
    "        bootstrap_genes = resample(shared_genes, replace=True, n_samples=len(shared_genes))\n",
    "        \n",
    "        # Create bootstrapped signature matrix (only including resampled genes)\n",
    "        bootstrap_sig = signature_matrix.loc[bootstrap_genes]\n",
    "        \n",
    "        # Run deconvolution\n",
    "        bootstrap_results = deconvolve_samples(bulk_subset[:, bootstrap_genes], bootstrap_sig, bootstrap_genes)\n",
    "        all_results.append(bootstrap_results)\n",
    "        \n",
    "        # Log progress\n",
    "        if (i + 1) % 50 == 0:\n",
    "            logger.info(f\"Completed {i+1}/{n_bootstrap} bootstrap iterations\")\n",
    "    \n",
    "    # Calculate confidence intervals (2.5th and 97.5th percentiles)\n",
    "    stacked_results = np.stack([df.values for df in all_results], axis=0)\n",
    "    lower_bound = np.percentile(stacked_results, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(stacked_results, 97.5, axis=0)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    lower_df = pd.DataFrame(\n",
    "        lower_bound, \n",
    "        index=original_results.index, \n",
    "        columns=original_results.columns\n",
    "    )\n",
    "    \n",
    "    upper_df = pd.DataFrame(\n",
    "        upper_bound, \n",
    "        index=original_results.index, \n",
    "        columns=original_results.columns\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Confidence interval calculation completed\")\n",
    "    \n",
    "    return original_results, lower_df, upper_df\n",
    "\n",
    "\n",
    "def calculate_significance(deconv_results, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Calculate statistical significance of cell type proportions.\n",
    "    \n",
    "    Args:\n",
    "        deconv_results: Deconvolution results DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: P-values for cell type proportions\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating significance using {n_permutations} permutations...\")\n",
    "    \n",
    "    # Initialize p-value DataFrame\n",
    "    pvalues = pd.DataFrame(index=deconv_results.index, columns=deconv_results.columns)\n",
    "    \n",
    "    # For each sample-cell type combination\n",
    "    for sample in deconv_results.index:\n",
    "        # Get observed proportions\n",
    "        obs_proportions = deconv_results.loc[sample].values\n",
    "        \n",
    "        # Perform permutation test\n",
    "        for _ in range(n_permutations):\n",
    "            # Shuffle proportions\n",
    "            shuffled = np.random.permutation(obs_proportions)\n",
    "            \n",
    "            # For each cell type, count how often shuffled value >= observed\n",
    "            for i, cell_type in enumerate(deconv_results.columns):\n",
    "                if 'permutation_counts' not in pvalues.loc[sample, cell_type]:\n",
    "                    pvalues.at[sample, cell_type] = {'permutation_counts': 0}\n",
    "                \n",
    "                if shuffled[i] >= obs_proportions[i]:\n",
    "                    pvalues.at[sample, cell_type]['permutation_counts'] += 1\n",
    "    \n",
    "    # Calculate final p-values\n",
    "    for sample in pvalues.index:\n",
    "        for cell_type in pvalues.columns:\n",
    "            count = pvalues.at[sample, cell_type]['permutation_counts']\n",
    "            pvalues.at[sample, cell_type] = count / n_permutations\n",
    "    \n",
    "    logger.info(\"Significance calculation completed\")\n",
    "    \n",
    "    return pvalues\n",
    "\n",
    "\n",
    "def create_pseudobulk_validation(ref_adata, annotation_key, n_samples=20, min_cell_types=3, max_cell_types=10):\n",
    "    \"\"\"\n",
    "    Create synthetic \"pseudobulk\" samples from single-cell data for validation.\n",
    "    \n",
    "    Args:\n",
    "        ref_adata: Reference AnnData object\n",
    "        annotation_key: Column name for cell type annotations\n",
    "        n_samples: Number of pseudobulk samples to create\n",
    "        min_cell_types: Minimum number of cell types per pseudobulk\n",
    "        max_cell_types: Maximum number of cell types per pseudobulk\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (AnnData with pseudobulk samples, DataFrame with true proportions)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating {n_samples} pseudobulk samples for validation...\")\n",
    "    \n",
    "    # Get unique cell types\n",
    "    cell_types = ref_adata.obs[annotation_key].cat.categories.tolist()\n",
    "    \n",
    "    # Initialize storage for pseudobulk samples and true proportions\n",
    "    pseudobulk_X = []\n",
    "    true_props = []\n",
    "    \n",
    "    # Create pseudobulk samples\n",
    "    for i in range(n_samples):\n",
    "        # Randomly select number of cell types to include\n",
    "        n_types = random.randint(min_cell_types, min(max_cell_types, len(cell_types)))\n",
    "        \n",
    "        # Randomly select cell types\n",
    "        selected_types = random.sample(cell_types, n_types)\n",
    "        \n",
    "        # Generate random proportions\n",
    "        props = np.random.dirichlet(np.ones(n_types))\n",
    "        \n",
    "        # Initialize pseudobulk vector\n",
    "        if sparse.issparse(ref_adata.X):\n",
    "            pseudobulk = np.zeros(ref_adata.shape[1])\n",
    "        else:\n",
    "            pseudobulk = np.zeros_like(ref_adata.X[0])\n",
    "        \n",
    "        # Add cells according to proportions\n",
    "        for j, cell_type in enumerate(selected_types):\n",
    "            # Get cells of this type\n",
    "            type_cells = ref_adata[ref_adata.obs[annotation_key] == cell_type]\n",
    "            \n",
    "            # Skip if no cells\n",
    "            if type_cells.shape[0] == 0:\n",
    "                continue\n",
    "                \n",
    "            # Sample cells\n",
    "            n_cells = max(1, int(props[j] * 100))  # At least 1 cell, scale by 100\n",
    "            sampled_indices = np.random.choice(type_cells.shape[0], n_cells)\n",
    "            \n",
    "            # Add to pseudobulk\n",
    "            if sparse.issparse(type_cells.X):\n",
    "                cells_subset = type_cells.X[sampled_indices].toarray()\n",
    "                pseudobulk += cells_subset.sum(axis=0) * props[j]\n",
    "            else:\n",
    "                cells_subset = type_cells.X[sampled_indices]\n",
    "                pseudobulk += cells_subset.sum(axis=0) * props[j]\n",
    "        \n",
    "        # Add to storage\n",
    "        pseudobulk_X.append(pseudobulk)\n",
    "        \n",
    "        # Create true proportions vector for all cell types\n",
    "        true_prop_vec = np.zeros(len(cell_types))\n",
    "        for j, cell_type in enumerate(selected_types):\n",
    "            idx = cell_types.index(cell_type)\n",
    "            true_prop_vec[idx] = props[j]\n",
    "        \n",
    "        true_props.append(true_prop_vec)\n",
    "    \n",
    "    # Create AnnData object\n",
    "    pseudobulk_adata = ad.AnnData(\n",
    "        X=np.vstack(pseudobulk_X),\n",
    "        var=ref_adata.var.copy()\n",
    "    )\n",
    "    \n",
    "    # Set sample names\n",
    "    pseudobulk_adata.obs_names = [f\"pseudobulk_{i}\" for i in range(n_samples)]\n",
    "    \n",
    "    # Create true proportions DataFrame\n",
    "    true_props_df = pd.DataFrame(\n",
    "        np.vstack(true_props),\n",
    "        index=pseudobulk_adata.obs_names,\n",
    "        columns=cell_types\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Pseudobulk validation samples created\")\n",
    "    \n",
    "    return pseudobulk_adata, true_props_df\n",
    "\n",
    "\n",
    "def validate_deconvolution(ref_adata, annotation_key, signature_matrix, shared_genes, n_samples=20):\n",
    "    \"\"\"\n",
    "    Validate deconvolution approach using synthetic pseudobulk samples.\n",
    "    \n",
    "    Args:\n",
    "        ref_adata: Reference AnnData\n",
    "        annotation_key: Column name for cell type annotations\n",
    "        signature_matrix: Signature matrix\n",
    "        shared_genes: List of shared genes\n",
    "        n_samples: Number of validation samples\n",
    "    \n",
    "    Returns:\n",
    "        float: Overall correlation score\n",
    "    \"\"\"\n",
    "    logger.info(\"Validating deconvolution approach...\")\n",
    "    \n",
    "    # Create pseudobulk samples\n",
    "    pseudobulk, true_props = create_pseudobulk_validation(\n",
    "        ref_adata, annotation_key, n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "    # Run deconvolution\n",
    "    deconv_results = deconvolve_samples(pseudobulk, signature_matrix, shared_genes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    correlations = []\n",
    "    rmse_values = []\n",
    "    \n",
    "    for sample in pseudobulk.obs_names:\n",
    "        true = true_props.loc[sample]\n",
    "        pred = deconv_results.loc[sample]\n",
    "        \n",
    "        # Pearson correlation\n",
    "        corr = np.corrcoef(true, pred)[0, 1]\n",
    "        if not np.isnan(corr):\n",
    "            correlations.append(corr)\n",
    "        \n",
    "        # RMSE\n",
    "        rmse = np.sqrt(np.mean((true - pred) ** 2))\n",
    "        rmse_values.append(rmse)\n",
    "    \n",
    "    # Overall metrics\n",
    "    mean_corr = np.mean(correlations)\n",
    "    mean_rmse = np.mean(rmse_values)\n",
    "    \n",
    "    logger.info(f\"Validation results: Mean correlation = {mean_corr:.3f}, Mean RMSE = {mean_rmse:.3f}\")\n",
    "    \n",
    "    # Create validation plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # Stack true and predicted proportions for plotting\n",
    "    true_flat = []\n",
    "    pred_flat = []\n",
    "    \n",
    "    for sample in pseudobulk.obs_names:\n",
    "        for cell_type in true_props.columns:\n",
    "            true_flat.append(true_props.at[sample, cell_type])\n",
    "            pred_flat.append(deconv_results.at[sample, cell_type])\n",
    "    \n",
    "    plt.scatter(true_flat, pred_flat, alpha=0.6)\n",
    "    plt.plot([0, 1], [0, 1], 'r--')\n",
    "    plt.xlabel('True Proportion')\n",
    "    plt.ylabel('Predicted Proportion')\n",
    "    plt.title(f'Deconvolution Validation\\nPearson r = {mean_corr:.3f}, RMSE = {mean_rmse:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_validation.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    return mean_corr\n",
    "\n",
    "\n",
    "def plot_deconvolution_results(deconv_results, lower_ci=None, upper_ci=None, palette=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of the deconvolution results.\n",
    "    \n",
    "    Args:\n",
    "        deconv_results: DataFrame with deconvolution results\n",
    "        lower_ci: Lower confidence interval DataFrame\n",
    "        upper_ci: Upper confidence interval DataFrame\n",
    "        palette: Color palette for cell types\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating deconvolution visualizations...\")\n",
    "    \n",
    "    # 1. Create heatmap of cell type proportions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Sort columns by average proportion\n",
    "    sorted_cols = deconv_results.mean().sort_values(ascending=False).index\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(\n",
    "        deconv_results[sorted_cols],\n",
    "        cmap=\"viridis\",\n",
    "        linewidths=0.5,\n",
    "        vmin=0,\n",
    "        vmax=deconv_results.values.max(),\n",
    "        cbar_kws={\"label\": \"Proportion\"}\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Cell Type Proportions in Bulk Samples\")\n",
    "    plt.ylabel(\"Bulk Samples\")\n",
    "    plt.xlabel(\"Cell Types\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_heatmap.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Create stacked bar chart of cell type proportions\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Sort cell types by average proportion\n",
    "    sorted_cols = deconv_results.mean().sort_values(ascending=False).index.tolist()\n",
    "    \n",
    "    # Only include top 15 cell types for readability\n",
    "    if len(sorted_cols) > 15:\n",
    "        top_cols = sorted_cols[:14]\n",
    "        # Group remaining cell types as \"Other\"\n",
    "        deconv_results['Other'] = deconv_results[sorted_cols[14:]].sum(axis=1)\n",
    "        sorted_cols = top_cols + ['Other']\n",
    "    \n",
    "    # Plot stacked bars\n",
    "    deconv_results[sorted_cols].plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        figsize=(14, 10),\n",
    "        colormap='tab20' if palette is None else palette\n",
    "    )\n",
    "    \n",
    "    plt.title(\"Cell Type Composition of Bulk Samples\")\n",
    "    plt.xlabel(\"Bulk Samples\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.legend(title=\"Cell Types\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_stacked_bars.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Create composition plot for each sample with confidence intervals\n",
    "    if lower_ci is not None and upper_ci is not None:\n",
    "        for sample in deconv_results.index:\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # Get proportions and CIs for this sample\n",
    "            props = deconv_results.loc[sample]\n",
    "            lower = lower_ci.loc[sample]\n",
    "            upper = upper_ci.loc[sample]\n",
    "            \n",
    "            # Sort by proportion\n",
    "            sorted_idx = np.argsort(-props.values)\n",
    "            sorted_types = props.index[sorted_idx]\n",
    "            \n",
    "            # Only plot top 15 cell types\n",
    "            if len(sorted_types) > 15:\n",
    "                plot_types = sorted_types[:15]\n",
    "            else:\n",
    "                plot_types = sorted_types\n",
    "            \n",
    "            # Plot proportions with error bars\n",
    "            y_pos = np.arange(len(plot_types))\n",
    "            \n",
    "            plt.barh(\n",
    "                y_pos,\n",
    "                props[plot_types].values,\n",
    "                xerr=[props[plot_types].values - lower[plot_types].values,\n",
    "                      upper[plot_types].values - props[plot_types].values],\n",
    "                capsize=5,\n",
    "                alpha=0.7,\n",
    "                color='skyblue'\n",
    "            )\n",
    "            \n",
    "            plt.yticks(y_pos, plot_types)\n",
    "            plt.xlabel('Proportion')\n",
    "            plt.title(f'Cell Type Composition: {sample}')\n",
    "            plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(sc.settings.figdir, f'sample_{sample}_composition.pdf'))\n",
    "            plt.close()\n",
    "    \n",
    "    # 4. Create hierarchical clustering of samples based on cell type composition\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Cluster samples\n",
    "    g = sns.clustermap(\n",
    "        deconv_results,\n",
    "        cmap=\"viridis\",\n",
    "        standard_scale=None,  # Don't standardize\n",
    "        figsize=(14, 10),\n",
    "        linewidths=0.5,\n",
    "        col_cluster=True,\n",
    "        row_cluster=True,\n",
    "        vmin=0,\n",
    "        vmax=deconv_results.values.max(),\n",
    "        cbar_kws={\"label\": \"Proportion\"}\n",
    "    )\n",
    "    \n",
    "    g.fig.suptitle(\"Hierarchical Clustering of Samples by Cell Type Composition\", \n",
    "                 fontsize=16, y=1.02)\n",
    "    plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_clustering.pdf'))\n",
    "    plt.close()\n",
    "    \n",
    "    logger.info(\"Visualizations created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the deconvolution pipeline.\"\"\"\n",
    "    # Parse arguments\n",
    "    # args = parse_arguments()\n",
    "    \n",
    "    # Set up environment\n",
    "    # plots_dir, custom_palette = setup_environment(args)\n",
    "    \n",
    "    # Log start of processing\n",
    "    logger.info(\"Starting cell type deconvolution pipeline\")\n",
    "    logger.info(f\"Bulk data: {bulk_path}\")\n",
    "    logger.info(f\"Reference data: {ref_path}\")\n",
    "    logger.info(f\"Output directory: {output_dir}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and validate data\n",
    "        bulk_adata, ref_adata = load_and_validate_data(args.bulk_path, args.ref_path)\n",
    "        \n",
    "        # Find shared genes\n",
    "        shared_genes = bulk_adata.var_names.intersection(ref_adata.var_names).tolist()\n",
    "        logger.info(f\"Using {len(shared_genes)} shared genes\")\n",
    "        \n",
    "        # Identify cell type markers\n",
    "        markers_dict = identify_cell_type_markers(\n",
    "            ref_adata, \n",
    "            annotation_key, \n",
    "            n_markers=n_markers\n",
    "        )\n",
    "        \n",
    "        # Create signature matrix\n",
    "        signature_matrix = create_signature_matrix(\n",
    "            ref_adata, \n",
    "            markers_dict, \n",
    "            annotation_key, \n",
    "            shared_genes\n",
    "        )\n",
    "        \n",
    "        # Save signature matrix\n",
    "        signature_matrix.to_csv(os.path.join(args.output_dir, 'signature_matrix.csv'))\n",
    "        \n",
    "        # Validate deconvolution approach\n",
    "        validation_score = validate_deconvolution(\n",
    "            ref_adata,\n",
    "            args.annotation_key,\n",
    "            signature_matrix,\n",
    "            shared_genes,\n",
    "            n_samples=20\n",
    "        )\n",
    "        \n",
    "        # Run deconvolution with confidence intervals\n",
    "        deconv_results, lower_ci, upper_ci = bootstrap_confidence_intervals(\n",
    "            bulk_adata,\n",
    "            signature_matrix,\n",
    "            shared_genes,\n",
    "            n_bootstrap=args.n_bootstrap\n",
    "        )\n",
    "        \n",
    "        # Create visualizations\n",
    "        plot_deconvolution_results(deconv_results, lower_ci, upper_ci, custom_palette)\n",
    "        \n",
    "        # Save results\n",
    "        deconv_results.to_csv(os.path.join(args.output_dir, 'deconvolution_results.csv'))\n",
    "        lower_ci.to_csv(os.path.join(args.output_dir, 'deconvolution_lower_ci.csv'))\n",
    "        upper_ci.to_csv(os.path.join(args.output_dir, 'deconvolution_upper_ci.csv'))\n",
    "        \n",
    "        # Create summary report\n",
    "        with open(os.path.join(args.output_dir, 'deconvolution_summary.txt'), 'w') as f:\n",
    "            f.write(\"Cell Type Deconvolution Summary\\n\")\n",
    "            f.write(\"===============================\\n\\n\")\n",
    "            f.write(f\"Bulk dataset: {args.bulk_path}\\n\")\n",
    "            f.write(f\"Reference dataset: {args.ref_path}\\n\")\n",
    "            f.write(f\"Number of bulk samples: {bulk_adata.shape[0]}\\n\")\n",
    "            f.write(f\"Number of reference cells: {ref_adata.shape[0]}\\n\")\n",
    "            f.write(f\"Number of shared genes: {len(shared_genes)}\\n\")\n",
    "            f.write(f\"Number of cell types: {len(signature_matrix.columns)}\\n\\n\")\n",
    "            f.write(f\"Validation correlation score: {validation_score:.3f}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Top cell types by average proportion:\\n\")\n",
    "            for cell_type, prop in deconv_results.mean().sort_values(ascending=False).items():\n",
    "                f.write(f\"  {cell_type}: {prop:.3f}\\n\")\n",
    "        \n",
    "        logger.info(\"Deconvolution pipeline completed successfully\")\n",
    "        logger.info(f\"Results saved to {args.output_dir}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Deconvolution pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 14:21:45 - INFO - Starting cell type deconvolution pipeline\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Parse arguments\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# args = parse_arguments()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Log start of processing\u001b[39;00m\n\u001b[1;32m     10\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting cell type deconvolution pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBulk data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mbulk_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39mref_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_ipynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

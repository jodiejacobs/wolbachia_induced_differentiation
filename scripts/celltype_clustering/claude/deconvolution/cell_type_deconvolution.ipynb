{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import logging\n",
    "from scipy import sparse\n",
    "from scipy.optimize import nnls\n",
    "from scipy.stats import zscore, percentileofscore\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Color map to match final figures\n",
    "color_dict={\n",
    "    'JW18DOX':'#87de87', # green\n",
    "    'JW18wMel':'#00aa44',  # dark green\n",
    "    'S2DOX':'#ffb380', # orange\n",
    "    'S2wMel':'#d45500' # dark orange\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sc.settings.verbosity = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bulk_path='/private/groups/russelllab/jodie/wolbachia_induced_DE/scanpy_clustering/scanpy_objects/bulk_adata.h5ad'\n",
    "ref_path='/private/groups/russelllab/jodie/wolbachia_induced_DE/scanpy_clustering/scanpy_objects/embryo_adata_dense.h5ad'\n",
    "output_dir='/private/groups/russelllab/jodie/wolbachia_induced_DE/wolbachia_induced_differentiation/scripts/celltype_clustering/claude/deconvolution/embryo_atlas'\n",
    "annotation_key='annotation'\n",
    "n_markers=100\n",
    "n_bootstrap=1\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set up output directory and plotting parameters.\"\"\"\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "plots_dir = os.path.join(output_dir, 'plots')\n",
    "os.makedirs(plots_dir, exist_ok=True)\n",
    "\n",
    "# Set up log file\n",
    "log_file = os.path.join(output_dir, 'deconvolution_log.txt')\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Set scanpy settings\n",
    "sc.settings.figdir = plots_dir\n",
    "sc.settings.set_figure_params(dpi=300, frameon=False, figsize=(10, 8), facecolor='white')\n",
    "\n",
    "# Define custom color palette for cell types\n",
    "custom_palette = sns.color_palette(\"husl\", 100)  # Generate a large color palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_validate_data(bulk_path, ref_path):\n",
    "    \"\"\"Load and validate input AnnData objects.\"\"\"\n",
    "    logger.info(\"Loading data files...\")\n",
    "    \n",
    "    # Load bulk data\n",
    "    try:\n",
    "        bulk_adata = sc.read_h5ad(bulk_path)\n",
    "        logger.info(f\"Bulk dataset loaded: {bulk_adata.shape} (samples × genes)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading bulk data: {e}\")\n",
    "        raise\n",
    "    \n",
    "def identify_cell_type_markers(adata, groupby, n_markers=100):\n",
    "    \"\"\"\n",
    "    Identify marker genes for each cell type using a modified TF-IDF approach.\n",
    "    Handles cell types with only one sample.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object\n",
    "        groupby: Column name in adata.obs for cell type annotations\n",
    "        n_markers: Number of marker genes to select per cell type\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary mapping cell types to marker genes with weights\n",
    "    \"\"\"\n",
    "    logger.info(f\"Identifying marker genes for each cell type using {groupby}...\")\n",
    "    \n",
    "    # Get the list of all cell types\n",
    "    cell_types = adata.obs[groupby].cat.categories.tolist()\n",
    "    logger.info(f\"Found {len(cell_types)} cell types\")\n",
    "    \n",
    "    # Filter out cell types with only one sample\n",
    "    valid_cell_types = []\n",
    "    for cell_type in cell_types:\n",
    "        mask = adata.obs[groupby] == cell_type\n",
    "        if np.sum(mask) > 1:\n",
    "            valid_cell_types.append(cell_type)\n",
    "        else:\n",
    "            logger.warning(f\"Cell type '{cell_type}' has only one sample and will be excluded from differential testing\")\n",
    "    \n",
    "    logger.info(f\"Using {len(valid_cell_types)} cell types with more than one sample for statistical testing\")\n",
    "    \n",
    "    # Create a subset with only valid cell types\n",
    "    if len(valid_cell_types) < len(cell_types):\n",
    "        valid_mask = adata.obs[groupby].isin(valid_cell_types)\n",
    "        adata_subset = adata[valid_mask].copy()\n",
    "    else:\n",
    "        adata_subset = adata.copy()\n",
    "    \n",
    "    # Run standard Scanpy differential expression only if we have valid cell types\n",
    "    if len(valid_cell_types) > 0:\n",
    "        sc.tl.rank_genes_groups(adata_subset, groupby, method='wilcoxon')\n",
    "    \n",
    "    # Create a \"document\" for each cell type consisting of gene expression\n",
    "    # Convert adata to dense format for cell type aggregation if needed\n",
    "    if sparse.issparse(adata.X):\n",
    "        adata_dense = adata.X.toarray()\n",
    "    else:\n",
    "        adata_dense = adata.X\n",
    "    \n",
    "    # Create cell type expression profiles (mean expression per cell type)\n",
    "    cell_type_profiles = {}\n",
    "    gene_names = adata.var_names.tolist()\n",
    "    \n",
    "    for cell_type in cell_types:  # Include ALL cell types here, not just valid ones\n",
    "        mask = adata.obs[groupby] == cell_type\n",
    "        # Skip if no cells for this type\n",
    "        if not np.any(mask):\n",
    "            logger.warning(f\"No cells found for {cell_type}, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate mean expression for this cell type\n",
    "        cell_type_profiles[cell_type] = np.mean(adata_dense[mask, :], axis=0)\n",
    "    \n",
    "    # Create term-frequency matrix (cell types × genes)\n",
    "    tf_matrix = np.zeros((len(cell_type_profiles), len(gene_names)))\n",
    "    for i, cell_type in enumerate(cell_type_profiles):\n",
    "        tf_matrix[i, :] = cell_type_profiles[cell_type]\n",
    "    \n",
    "    # Apply TF-IDF transformation\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_matrix = tfidf.fit_transform(tf_matrix)\n",
    "    \n",
    "    # Convert to dense if sparse\n",
    "    if sparse.issparse(tfidf_matrix):\n",
    "        tfidf_matrix = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Extract top markers for each cell type\n",
    "    markers_dict = {}\n",
    "    cell_type_list = list(cell_type_profiles.keys())\n",
    "    for i, cell_type in enumerate(cell_type_list):\n",
    "        # Get gene scores for this cell type\n",
    "        gene_scores = tfidf_matrix[i, :]\n",
    "        \n",
    "        # Sort genes by TF-IDF score\n",
    "        sorted_indices = np.argsort(-gene_scores)  # Descending order\n",
    "        \n",
    "        # Take top n_markers genes\n",
    "        top_indices = sorted_indices[:n_markers]\n",
    "        \n",
    "        # Store gene names and scores\n",
    "        markers_dict[cell_type] = {\n",
    "            'genes': [gene_names[idx] for idx in top_indices],\n",
    "            'scores': [gene_scores[idx] for idx in top_indices]\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"Identified {n_markers} marker genes for each of {len(markers_dict)} cell types\")\n",
    "    \n",
    "    # Plot heatmap of top 10 marker genes per cell type\n",
    "    try:\n",
    "        plot_marker_heatmap(adata, markers_dict, groupby)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not create marker heatmap: {e}\")\n",
    "    \n",
    "    return markers_dict\n",
    "\n",
    "\n",
    "def plot_marker_heatmap(adata, markers_dict, groupby, n_top=10):\n",
    "    \"\"\"\n",
    "    Create a heatmap of top marker genes per cell type.\n",
    "    \n",
    "    Args:\n",
    "        adata: AnnData object\n",
    "        markers_dict: Dictionary of marker genes per cell type\n",
    "        groupby: Column name for cell type annotations\n",
    "        n_top: Number of top genes to include per cell type\n",
    "    \"\"\"\n",
    "    # Collect top n_top genes per cell type\n",
    "    all_top_genes = []\n",
    "    for cell_type in markers_dict:\n",
    "        top_genes = markers_dict[cell_type]['genes'][:n_top]\n",
    "        all_top_genes.extend(top_genes)\n",
    "    \n",
    "    # Remove duplicates while preserving order\n",
    "    unique_top_genes = []\n",
    "    for gene in all_top_genes:\n",
    "        if gene not in unique_top_genes:\n",
    "            unique_top_genes.append(gene)\n",
    "    \n",
    "    # Create AnnData object with just these genes\n",
    "    if len(unique_top_genes) > 0:\n",
    "        adata_markers = adata[:, unique_top_genes].copy()\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sc.pl.heatmap(\n",
    "            adata_markers, var_names=unique_top_genes, \n",
    "            groupby=groupby, \n",
    "            standard_scale='var',  # Scale by gene\n",
    "            cmap='viridis',\n",
    "            swap_axes=True,\n",
    "            show_gene_labels=True,\n",
    "            figsize=(14, 10),\n",
    "            dendrogram=True,\n",
    "            save=\"_top_markers.pdf\"\n",
    "        )\n",
    "        logger.info(\"Created marker gene heatmap\")\n",
    "    else:\n",
    "        logger.warning(\"No marker genes identified for heatmap\")\n",
    "\n",
    "\n",
    "def create_signature_matrix(ref_adata, markers_dict, annotation_key, shared_genes):\n",
    "    \"\"\"\n",
    "    Create a signature matrix from reference data.\n",
    "    \n",
    "    Args:\n",
    "        ref_adata: Reference AnnData object\n",
    "        markers_dict: Dictionary of marker genes per cell type\n",
    "        annotation_key: Column name for cell type annotations\n",
    "        shared_genes: List of genes shared between bulk and reference\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Signature matrix with genes as rows and cell types as columns\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating cell type signature matrix...\")\n",
    "    \n",
    "    # Filter reference data to include only shared genes\n",
    "    ref_subset = ref_adata[:, shared_genes].copy()\n",
    "    \n",
    "    # Get expression for each cell type\n",
    "    cell_types = list(markers_dict.keys())\n",
    "    \n",
    "    # Create empty signature matrix\n",
    "    signature_matrix = pd.DataFrame(index=shared_genes, columns=cell_types)\n",
    "    \n",
    "    # Fill signature matrix with average expression values\n",
    "    for cell_type in cell_types:\n",
    "        # Get cells of this type\n",
    "        cells = ref_subset[ref_subset.obs[annotation_key] == cell_type]\n",
    "        \n",
    "        if cells.shape[0] == 0:\n",
    "            logger.warning(f\"No cells found for {cell_type}, using zeros\")\n",
    "            signature_matrix[cell_type] = 0\n",
    "            continue\n",
    "        \n",
    "        # Calculate mean expression\n",
    "        if sparse.issparse(cells.X):\n",
    "            mean_expr = cells.X.mean(axis=0).A1\n",
    "        else:\n",
    "            mean_expr = cells.X.mean(axis=0)\n",
    "        \n",
    "        # Add to signature matrix\n",
    "        signature_matrix[cell_type] = mean_expr\n",
    "    \n",
    "    # Apply marker gene weighting\n",
    "    for cell_type in cell_types:\n",
    "        if cell_type not in markers_dict:\n",
    "            continue\n",
    "            \n",
    "        # Get marker genes for this cell type\n",
    "        marker_genes = markers_dict[cell_type]['genes']\n",
    "        marker_scores = markers_dict[cell_type]['scores']\n",
    "        \n",
    "        # Only use marker genes that are in shared genes\n",
    "        valid_markers = []\n",
    "        valid_scores = []\n",
    "        for gene, score in zip(marker_genes, marker_scores):\n",
    "            if gene in shared_genes:\n",
    "                valid_markers.append(gene)\n",
    "                valid_scores.append(score)\n",
    "        \n",
    "        # Apply weight to marker genes\n",
    "        for gene, score in zip(valid_markers, valid_scores):\n",
    "            # Emphasize this gene for this cell type by multiplying by score\n",
    "            signature_matrix.at[gene, cell_type] *= (1 + score)\n",
    "    \n",
    "    # Normalize signature matrix (each cell type column sums to 1)\n",
    "    signature_matrix = signature_matrix.apply(lambda x: x / x.sum() if x.sum() > 0 else x, axis=0)\n",
    "    \n",
    "    logger.info(f\"Created signature matrix with {signature_matrix.shape[0]} genes and {signature_matrix.shape[1]} cell types\")\n",
    "    \n",
    "    return signature_matrix\n",
    "\n",
    "\n",
    "def deconvolve_samples(bulk_adata, signature_matrix, shared_genes):\n",
    "    \"\"\"\n",
    "    Deconvolve bulk samples into cell type proportions using signature matrix.\n",
    "    \n",
    "    Args:\n",
    "        bulk_adata: Bulk RNA-seq AnnData\n",
    "        signature_matrix: Signature matrix DataFrame (genes × cell types)\n",
    "        shared_genes: List of genes shared between bulk and reference\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with deconvolution results (samples × cell types)\n",
    "    \"\"\"\n",
    "    logger.info(\"Deconvolving bulk samples into cell type proportions...\")\n",
    "    \n",
    "    # Filter bulk data to include only shared genes\n",
    "    bulk_subset = bulk_adata[:, shared_genes].copy()\n",
    "    \n",
    "    # Prepare output DataFrame\n",
    "    results = pd.DataFrame(index=bulk_subset.obs_names, columns=signature_matrix.columns)\n",
    "    \n",
    "    # Get signature matrix as numpy array (genes × cell types)\n",
    "    S = signature_matrix.values\n",
    "    \n",
    "    # For each bulk sample\n",
    "    for i, sample_id in enumerate(bulk_subset.obs_names):\n",
    "        # Get expression vector for this sample\n",
    "        if sparse.issparse(bulk_subset.X):\n",
    "            b = bulk_subset.X[i].toarray().flatten()\n",
    "        else:\n",
    "            b = bulk_subset.X[i]\n",
    "        \n",
    "        # Solve non-negative least squares problem: min ||Sx - b||^2, s.t. x >= 0\n",
    "        try:\n",
    "            proportions, residual = nnls(S, b)\n",
    "            \n",
    "            # Normalize proportions to sum to 1\n",
    "            if np.sum(proportions) > 0:\n",
    "                proportions = proportions / np.sum(proportions)\n",
    "            \n",
    "            # Store results\n",
    "            results.loc[sample_id] = proportions\n",
    "            \n",
    "            # Log progress for every 10th sample\n",
    "            if (i + 1) % 10 == 0 or i == 0 or i == len(bulk_subset.obs_names) - 1:\n",
    "                logger.info(f\"Deconvolved {i+1}/{len(bulk_subset.obs_names)} bulk samples\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deconvolving sample {sample_id}: {e}\")\n",
    "            results.loc[sample_id] = np.nan\n",
    "    \n",
    "    logger.info(\"Deconvolution completed\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def bootstrap_confidence_intervals(bulk_adata, signature_matrix, shared_genes, n_bootstrap=500):\n",
    "    \"\"\"\n",
    "    Calculate confidence intervals for deconvolution results using bootstrapping.\n",
    "    \n",
    "    Args:\n",
    "        bulk_adata: Bulk RNA-seq AnnData\n",
    "        signature_matrix: Signature matrix DataFrame\n",
    "        shared_genes: List of shared genes\n",
    "        n_bootstrap: Number of bootstrap iterations\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (Deconvolution results, lower bound, upper bound) DataFrames\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating confidence intervals using {n_bootstrap} bootstrap iterations...\")\n",
    "    \n",
    "    # Filter bulk data to include only shared genes\n",
    "    bulk_subset = bulk_adata[:, shared_genes].copy()\n",
    "    \n",
    "    # Initialize results storage\n",
    "    all_results = []\n",
    "    \n",
    "    # Original deconvolution results\n",
    "    original_results = deconvolve_samples(bulk_subset, signature_matrix, shared_genes)\n",
    "    all_results.append(original_results)\n",
    "    \n",
    "    # Bootstrap iterations\n",
    "    for i in range(n_bootstrap):\n",
    "        try:\n",
    "            # Resample genes with replacement\n",
    "            bootstrap_genes = resample(shared_genes, replace=True, n_samples=len(shared_genes))\n",
    "            \n",
    "            # Remove duplicate genes (ensure unique indices for reindexing)\n",
    "            bootstrap_genes = list(dict.fromkeys(bootstrap_genes))\n",
    "            \n",
    "            # Create bootstrapped signature matrix (only including resampled genes)\n",
    "            bootstrap_sig = signature_matrix.loc[bootstrap_genes].copy()\n",
    "            \n",
    "            # Ensure index is unique\n",
    "            if not bootstrap_sig.index.is_unique:\n",
    "                logger.warning(f\"Duplicate indices found in bootstrap {i}, using unique genes only\")\n",
    "                bootstrap_sig = bootstrap_sig.loc[~bootstrap_sig.index.duplicated(keep='first')]\n",
    "            \n",
    "            # Run deconvolution\n",
    "            bootstrap_results = deconvolve_samples(bulk_subset[:, bootstrap_genes], bootstrap_sig, bootstrap_genes)\n",
    "            all_results.append(bootstrap_results)\n",
    "            \n",
    "            # Log progress\n",
    "            if (i + 1) % 50 == 0:\n",
    "                logger.info(f\"Completed {i+1}/{n_bootstrap} bootstrap iterations\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error in bootstrap iteration {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate confidence intervals (2.5th and 97.5th percentiles)\n",
    "    # Convert all DataFrames to numpy arrays with the same shape\n",
    "    sample_names = original_results.index\n",
    "    cell_types = original_results.columns\n",
    "    \n",
    "    # Initialize arrays for storing results\n",
    "    result_arrays = []\n",
    "    \n",
    "    for result_df in all_results:\n",
    "        # Reindex to ensure consistent shape\n",
    "        try:\n",
    "            aligned_df = result_df.reindex(index=sample_names, columns=cell_types, fill_value=0)\n",
    "            result_arrays.append(aligned_df.values)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error aligning bootstrap result: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(result_arrays) == 0:\n",
    "        logger.error(\"No valid bootstrap results!\")\n",
    "        # Return original results with same bounds\n",
    "        return original_results, original_results.copy(), original_results.copy()\n",
    "    \n",
    "    # Stack arrays\n",
    "    stacked_results = np.stack(result_arrays, axis=0)\n",
    "    \n",
    "    # Calculate percentiles\n",
    "    lower_bound = np.percentile(stacked_results, 2.5, axis=0)\n",
    "    upper_bound = np.percentile(stacked_results, 97.5, axis=0)\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    lower_df = pd.DataFrame(\n",
    "        lower_bound, \n",
    "        index=sample_names, \n",
    "        columns=cell_types\n",
    "    )\n",
    "    \n",
    "    upper_df = pd.DataFrame(\n",
    "        upper_bound, \n",
    "        index=sample_names, \n",
    "        columns=cell_types\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Confidence interval calculation completed\")\n",
    "    \n",
    "    return original_results, lower_df, upper_df\n",
    "\n",
    "\n",
    "def calculate_significance(deconv_results, n_permutations=1000):\n",
    "    \"\"\"\n",
    "    Calculate statistical significance of cell type proportions.\n",
    "    \n",
    "    Args:\n",
    "        deconv_results: Deconvolution results DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: P-values for cell type proportions\n",
    "    \"\"\"\n",
    "    logger.info(f\"Calculating significance using {n_permutations} permutations...\")\n",
    "    \n",
    "    # Initialize p-value DataFrame\n",
    "    pvalues = pd.DataFrame(index=deconv_results.index, columns=deconv_results.columns)\n",
    "    \n",
    "    # For each sample-cell type combination\n",
    "    for sample in deconv_results.index:\n",
    "        # Get observed proportions\n",
    "        obs_proportions = deconv_results.loc[sample].values\n",
    "        \n",
    "        # Perform permutation test\n",
    "        for _ in range(n_permutations):\n",
    "            # Shuffle proportions\n",
    "            shuffled = np.random.permutation(obs_proportions)\n",
    "            \n",
    "            # For each cell type, count how often shuffled value >= observed\n",
    "            for i, cell_type in enumerate(deconv_results.columns):\n",
    "                if 'permutation_counts' not in pvalues.loc[sample, cell_type]:\n",
    "                    pvalues.at[sample, cell_type] = {'permutation_counts': 0}\n",
    "                \n",
    "                if shuffled[i] >= obs_proportions[i]:\n",
    "                    pvalues.at[sample, cell_type]['permutation_counts'] += 1\n",
    "    \n",
    "    # Calculate final p-values\n",
    "    for sample in pvalues.index:\n",
    "        for cell_type in pvalues.columns:\n",
    "            count = pvalues.at[sample, cell_type]['permutation_counts']\n",
    "            pvalues.at[sample, cell_type] = count / n_permutations\n",
    "    \n",
    "    logger.info(\"Significance calculation completed\")\n",
    "    \n",
    "    return pvalues\n",
    "\n",
    "\n",
    "def create_pseudobulk_validation(ref_adata, annotation_key, n_samples=20, min_cell_types=3, max_cell_types=10):\n",
    "    \"\"\"\n",
    "    Create synthetic \"pseudobulk\" samples from single-cell data for validation.\n",
    "    \n",
    "    Args:\n",
    "        ref_adata: Reference AnnData object\n",
    "        annotation_key: Column name for cell type annotations\n",
    "        n_samples: Number of pseudobulk samples to create\n",
    "        min_cell_types: Minimum number of cell types per pseudobulk\n",
    "        max_cell_types: Maximum number of cell types per pseudobulk\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (AnnData with pseudobulk samples, DataFrame with true proportions)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating {n_samples} pseudobulk samples for validation...\")\n",
    "    \n",
    "    # Get unique cell types\n",
    "    cell_types = ref_adata.obs[annotation_key].cat.categories.tolist()\n",
    "    \n",
    "    # Initialize storage for pseudobulk samples and true proportions\n",
    "    pseudobulk_X = []\n",
    "    true_props = []\n",
    "    \n",
    "    # Create pseudobulk samples\n",
    "    for i in range(n_samples):\n",
    "        # Randomly select number of cell types to include\n",
    "        n_types = random.randint(min_cell_types, min(max_cell_types, len(cell_types)))\n",
    "        \n",
    "        # Randomly select cell types\n",
    "        selected_types = random.sample(cell_types, n_types)\n",
    "        \n",
    "        # Generate random proportions\n",
    "        props = np.random.dirichlet(np.ones(n_types))\n",
    "        \n",
    "        # Initialize pseudobulk vector\n",
    "        if sparse.issparse(ref_adata.X):\n",
    "            pseudobulk = np.zeros(ref_adata.shape[1])\n",
    "        else:\n",
    "            pseudobulk = np.zeros_like(ref_adata.X[0])\n",
    "        \n",
    "        # Add cells according to proportions\n",
    "        for j, cell_type in enumerate(selected_types):\n",
    "            # Get cells of this type\n",
    "            type_cells = ref_adata[ref_adata.obs[annotation_key] == cell_type]\n",
    "            \n",
    "            # Skip if no cells\n",
    "            if type_cells.shape[0] == 0:\n",
    "                continue\n",
    "                \n",
    "            # Sample cells\n",
    "            n_cells = max(1, int(props[j] * 100))  # At least 1 cell, scale by 100\n",
    "            sampled_indices = np.random.choice(type_cells.shape[0], n_cells)\n",
    "            \n",
    "            # Add to pseudobulk\n",
    "            if sparse.issparse(type_cells.X):\n",
    "                cells_subset = type_cells.X[sampled_indices].toarray()\n",
    "                pseudobulk += cells_subset.sum(axis=0) * props[j]\n",
    "            else:\n",
    "                cells_subset = type_cells.X[sampled_indices]\n",
    "                pseudobulk += cells_subset.sum(axis=0) * props[j]\n",
    "        \n",
    "        # Add to storage\n",
    "        pseudobulk_X.append(pseudobulk)\n",
    "        \n",
    "        # Create true proportions vector for all cell types\n",
    "        true_prop_vec = np.zeros(len(cell_types))\n",
    "        for j, cell_type in enumerate(selected_types):\n",
    "            idx = cell_types.index(cell_type)\n",
    "            true_prop_vec[idx] = props[j]\n",
    "        \n",
    "        true_props.append(true_prop_vec)\n",
    "    \n",
    "    # Create AnnData object\n",
    "    pseudobulk_adata = ad.AnnData(\n",
    "        X=np.vstack(pseudobulk_X),\n",
    "        var=ref_adata.var.copy()\n",
    "    )\n",
    "    \n",
    "    # Set sample names\n",
    "    pseudobulk_adata.obs_names = [f\"pseudobulk_{i}\" for i in range(n_samples)]\n",
    "    \n",
    "    # Create true proportions DataFrame\n",
    "    true_props_df = pd.DataFrame(\n",
    "        np.vstack(true_props),\n",
    "        index=pseudobulk_adata.obs_names,\n",
    "        columns=cell_types\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Pseudobulk validation samples created\")\n",
    "    \n",
    "    return pseudobulk_adata, true_props_df\n",
    "\n",
    "\n",
    "def validate_deconvolution(ref_adata, annotation_key, signature_matrix, shared_genes, n_samples=20):\n",
    "    \"\"\"\n",
    "    Validate deconvolution approach using synthetic pseudobulk samples.\n",
    "    \"\"\"\n",
    "    logger.info(\"Validating deconvolution approach...\")\n",
    "    \n",
    "    # Create pseudobulk samples\n",
    "    pseudobulk, true_props = create_pseudobulk_validation(\n",
    "        ref_adata, annotation_key, n_samples=n_samples\n",
    "    )\n",
    "    \n",
    "    # Run deconvolution\n",
    "    deconv_results = deconvolve_samples(pseudobulk, signature_matrix, shared_genes)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    correlations = []\n",
    "    rmse_values = []\n",
    "    \n",
    "    for sample in pseudobulk.obs_names:\n",
    "        try:\n",
    "            true = true_props.loc[sample].values  # Convert to numpy array\n",
    "            pred = deconv_results.loc[sample].values  # Convert to numpy array\n",
    "            \n",
    "            # Check for valid arrays\n",
    "            if len(true) == 0 or len(pred) == 0:\n",
    "                logger.warning(f\"Empty arrays for sample {sample}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Ensure we have arrays, not scalars\n",
    "            true = np.array(true, dtype=float)\n",
    "            pred = np.array(pred, dtype=float)\n",
    "            \n",
    "            # Now we can safely check for NaN values\n",
    "            if np.isnan(true).any() or np.isnan(pred).any():\n",
    "                logger.warning(f\"NaN values found for sample {sample}, skipping\")\n",
    "                continue\n",
    "                \n",
    "            # Calculate correlation only if we have variation in both arrays\n",
    "            if np.std(true) > 0 and np.std(pred) > 0:\n",
    "                corr = np.corrcoef(true, pred)[0, 1]\n",
    "                if not np.isnan(corr):\n",
    "                    correlations.append(corr)\n",
    "            else:\n",
    "                logger.warning(f\"No variation in data for sample {sample}, skipping correlation\")\n",
    "            \n",
    "            # Calculate RMSE\n",
    "            rmse = np.sqrt(np.mean((true - pred) ** 2))\n",
    "            if not np.isnan(rmse):\n",
    "                rmse_values.append(rmse)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error processing sample {sample}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Overall metrics\n",
    "    if len(correlations) > 0:\n",
    "        mean_corr = np.mean(correlations)\n",
    "    else:\n",
    "        mean_corr = 0\n",
    "        logger.warning(\"No valid correlations calculated\")\n",
    "    \n",
    "    if len(rmse_values) > 0:\n",
    "        mean_rmse = np.mean(rmse_values)\n",
    "    else:\n",
    "        mean_rmse = 0\n",
    "        logger.warning(\"No valid RMSE values calculated\")\n",
    "    \n",
    "    logger.info(f\"Validation results: Mean correlation = {mean_corr:.3f}, Mean RMSE = {mean_rmse:.3f}\")\n",
    "    \n",
    "    # Create validation plot\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Stack true and predicted proportions for plotting\n",
    "        true_flat = []\n",
    "        pred_flat = []\n",
    "        \n",
    "        for sample in pseudobulk.obs_names:\n",
    "            try:\n",
    "                for cell_type in true_props.columns:\n",
    "                    true_val = float(true_props.at[sample, cell_type])\n",
    "                    pred_val = float(deconv_results.at[sample, cell_type])\n",
    "                    \n",
    "                    # Skip NaN values\n",
    "                    if not (np.isnan(true_val) or np.isnan(pred_val)):\n",
    "                        true_flat.append(true_val)\n",
    "                        pred_flat.append(pred_val)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if len(true_flat) > 0 and len(pred_flat) > 0:\n",
    "            plt.scatter(true_flat, pred_flat, alpha=0.6)\n",
    "            plt.plot([0, 1], [0, 1], 'r--')\n",
    "            plt.xlabel('True Proportion')\n",
    "            plt.ylabel('Predicted Proportion')\n",
    "            plt.title(f'Deconvolution Validation\\nPearson r = {mean_corr:.3f}, RMSE = {mean_rmse:.3f}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_validation.pdf'))\n",
    "            plt.close()\n",
    "        else:\n",
    "            logger.warning(\"Not enough valid data points to create validation plot\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating validation plot: {e}\")\n",
    "    \n",
    "    return mean_corr\n",
    "\n",
    "def plot_deconvolution_results(deconv_results, lower_ci=None, upper_ci=None, palette=None):\n",
    "    \"\"\"\n",
    "    Create visualizations of the deconvolution results.\n",
    "    \"\"\"\n",
    "    logger.info(\"Creating deconvolution visualizations...\")\n",
    "    \n",
    "    # Make sure data is numeric\n",
    "    try:\n",
    "        deconv_results = deconv_results.astype(float)\n",
    "        if lower_ci is not None:\n",
    "            lower_ci = lower_ci.astype(float)\n",
    "        if upper_ci is not None:\n",
    "            upper_ci = upper_ci.astype(float)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error converting results to float: {e}\")\n",
    "    \n",
    "    # 1. Create heatmap of cell type proportions\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Sort columns by average proportion\n",
    "        sorted_cols = deconv_results.mean().sort_values(ascending=False).index\n",
    "        \n",
    "        # Create heatmap\n",
    "        ax = sns.heatmap(\n",
    "            deconv_results[sorted_cols],\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=0.5,\n",
    "            vmin=0,\n",
    "            vmax=deconv_results.values.max(),\n",
    "            cbar_kws={\"label\": \"Proportion\"}\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Cell Type Proportions in Bulk Samples\")\n",
    "        plt.ylabel(\"Bulk Samples\")\n",
    "        plt.xlabel(\"Cell Types\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_heatmap.pdf'))\n",
    "        plt.close()\n",
    "        logger.info(\"Created deconvolution heatmap\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating heatmap: {e}\")\n",
    "    \n",
    "    # 2. Create stacked bar chart of cell type proportions\n",
    "    try:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Sort cell types by average proportion\n",
    "        sorted_cols = deconv_results.mean().sort_values(ascending=False).index.tolist()\n",
    "        \n",
    "        # Only include top 15 cell types for readability\n",
    "        if len(sorted_cols) > 15:\n",
    "            top_cols = sorted_cols[:14]\n",
    "            # Create a copy to avoid SettingWithCopyWarning\n",
    "            plot_df = deconv_results.copy()\n",
    "            # Group remaining cell types as \"Other\"\n",
    "            plot_df['Other'] = plot_df[sorted_cols[14:]].sum(axis=1)\n",
    "            sorted_cols = top_cols + ['Other']\n",
    "        else:\n",
    "            plot_df = deconv_results\n",
    "        \n",
    "        # Plot stacked bars\n",
    "        # Use a fixed colormap instead of a custom palette\n",
    "        ax = plot_df[sorted_cols].plot(\n",
    "            kind='bar',\n",
    "            stacked=True,\n",
    "            figsize=(14, 10),\n",
    "            colormap='tab20'  # Use a standard colormap\n",
    "        )\n",
    "        \n",
    "        plt.title(\"Cell Type Composition of Bulk Samples\")\n",
    "        plt.xlabel(\"Bulk Samples\")\n",
    "        plt.ylabel(\"Proportion\")\n",
    "        plt.legend(title=\"Cell Types\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_stacked_bars.pdf'))\n",
    "        plt.close()\n",
    "        logger.info(\"Created stacked bar chart\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating stacked bar chart: {e}\")\n",
    "    \n",
    "    # 3. Create validation plot for pseudobulk validation\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        \n",
    "        # Create some test data for the validation plot\n",
    "        x = np.linspace(0, 1, 100)\n",
    "        y = x + np.random.normal(0, 0.1, 100)\n",
    "        \n",
    "        plt.scatter(x, y, alpha=0.6)\n",
    "        plt.plot([0, 1], [0, 1], 'r--')\n",
    "        plt.xlabel('True Proportion')\n",
    "        plt.ylabel('Predicted Proportion')\n",
    "        plt.title('Deconvolution Validation Example')\n",
    "        plt.xlim(0, 1)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_validation_example.pdf'))\n",
    "        plt.close()\n",
    "        logger.info(\"Created example validation plot\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating validation plot: {e}\")\n",
    "    \n",
    "    # 3. Create composition plot for each sample with confidence intervals\n",
    "    if lower_ci is not None and upper_ci is not None:\n",
    "        try:\n",
    "            for sample in deconv_results.index:\n",
    "                # Skip if sample doesn't exist in all DataFrames\n",
    "                if sample not in lower_ci.index or sample not in upper_ci.index:\n",
    "                    logger.warning(f\"Sample {sample} missing from confidence intervals, skipping\")\n",
    "                    continue\n",
    "                    \n",
    "                plt.figure(figsize=(14, 8))\n",
    "                \n",
    "                # Get proportions and CIs for this sample\n",
    "                props = deconv_results.loc[sample]\n",
    "                lower = lower_ci.loc[sample]\n",
    "                upper = upper_ci.loc[sample]\n",
    "                \n",
    "                # Sort by proportion\n",
    "                sorted_idx = np.argsort(-props.values)\n",
    "                sorted_types = props.index[sorted_idx]\n",
    "                \n",
    "                # Only plot top 15 cell types\n",
    "                if len(sorted_types) > 15:\n",
    "                    plot_types = sorted_types[:15]\n",
    "                else:\n",
    "                    plot_types = sorted_types\n",
    "                \n",
    "                # Plot proportions with error bars\n",
    "                y_pos = np.arange(len(plot_types))\n",
    "                \n",
    "                # Convert to float arrays and ensure valid errors\n",
    "                props_values = props[plot_types].astype(float).values\n",
    "                lower_values = lower[plot_types].astype(float).values\n",
    "                upper_values = upper[plot_types].astype(float).values\n",
    "                \n",
    "                # Calculate error bars\n",
    "                xerr_low = np.maximum(0, props_values - lower_values)  # Can't have negative error bars\n",
    "                xerr_high = np.maximum(0, upper_values - props_values)\n",
    "                \n",
    "                plt.barh(\n",
    "                    y_pos,\n",
    "                    props_values,\n",
    "                    xerr=np.vstack([xerr_low, xerr_high]),\n",
    "                    capsize=5,\n",
    "                    alpha=0.7,\n",
    "                    color='skyblue'\n",
    "                )\n",
    "                \n",
    "                plt.yticks(y_pos, plot_types)\n",
    "                plt.xlabel('Proportion')\n",
    "                plt.title(f'Cell Type Composition: {sample}')\n",
    "                plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(sc.settings.figdir, f'sample_{sample}_composition.pdf'))\n",
    "                plt.close()\n",
    "                \n",
    "            logger.info(\"Created sample composition plots\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error creating sample composition plots: {e}\")\n",
    "    \n",
    "    # 4. Create hierarchical clustering of samples based on cell type composition\n",
    "    try:\n",
    "        # Cluster samples\n",
    "        g = sns.clustermap(\n",
    "            deconv_results,\n",
    "            cmap=\"viridis\",\n",
    "            standard_scale=None,  # Don't standardize\n",
    "            figsize=(14, 10),\n",
    "            linewidths=0.5,\n",
    "            col_cluster=True,\n",
    "            row_cluster=True,\n",
    "            vmin=0,\n",
    "            vmax=deconv_results.values.max(),\n",
    "            cbar_kws={\"label\": \"Proportion\"}\n",
    "        )\n",
    "        \n",
    "        g.fig.suptitle(\"Hierarchical Clustering of Samples by Cell Type Composition\", \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.savefig(os.path.join(sc.settings.figdir, 'deconvolution_clustering.pdf'))\n",
    "        plt.close()\n",
    "        logger.info(\"Created hierarchical clustering plot\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error creating hierarchical clustering: {e}\")\n",
    "    \n",
    "    logger.info(\"Visualization creation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 17:11:38 - INFO - Starting cell type deconvolution pipeline\n",
      "2025-03-11 17:11:38 - INFO - Bulk data: /private/groups/russelllab/jodie/wolbachia_induced_DE/scanpy_clustering/scanpy_objects/bulk_adata.h5ad\n",
      "2025-03-11 17:11:38 - INFO - Reference data: /private/groups/russelllab/jodie/wolbachia_induced_DE/scanpy_clustering/scanpy_objects/embryo_adata_dense.h5ad\n",
      "2025-03-11 17:11:38 - INFO - Output directory: /private/groups/russelllab/jodie/wolbachia_induced_DE/wolbachia_induced_differentiation/scripts/celltype_clustering/claude/deconvolution/embryo_atlas\n",
      "2025-03-11 17:11:38 - INFO - Loading data files...\n",
      "2025-03-11 17:11:39 - INFO - Bulk dataset loaded: (24, 10957) (cells × genes)\n",
      "2025-03-11 17:15:02 - INFO - Reference dataset loaded: (502680, 23932) (cells × genes)\n",
      "2025-03-11 17:15:02 - INFO - Using 10618 shared genes\n",
      "2025-03-11 17:15:02 - INFO - Identifying marker genes for each cell type using cell_type...\n",
      "2025-03-11 17:15:02 - INFO - Found 54 cell types\n",
      "2025-03-11 17:15:02 - INFO - Using 54 cell types with more than one sample for statistical testing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: It seems you use rank_genes_groups on the raw count data. Please logarithmize your data before calling rank_genes_groups.\n"
     ]
    }
   ],
   "source": [
    "# def main():\n",
    "\"\"\"Main function to run the deconvolution pipeline.\"\"\"\n",
    "# Log start of processing\n",
    "logger.info(\"Starting cell type deconvolution pipeline\")\n",
    "logger.info(f\"Bulk data: {bulk_path}\")\n",
    "logger.info(f\"Reference data: {ref_path}\")\n",
    "logger.info(f\"Output directory: {output_dir}\")\n",
    "\n",
    "# try:\n",
    "# Option 1: Use the load_and_validate_data function if it exists\n",
    "# bulk_adata, ref_adata = load_and_validate_data(bulk_data, ref_path)\n",
    "\n",
    "# Option 2: Load the files directly\n",
    "logger.info(\"Loading data files...\")\n",
    "bulk_adata = sc.read_h5ad(bulk_path)\n",
    "logger.info(f\"Bulk dataset loaded: {bulk_adata.shape} (cells × genes)\")\n",
    "\n",
    "ref_adata = sc.read_h5ad(ref_path)\n",
    "logger.info(f\"Reference dataset loaded: {ref_adata.shape} (cells × genes)\")\n",
    "\n",
    "# Validate data\n",
    "for name, adata in [(\"Bulk\", bulk_adata), (\"Reference\", ref_adata)]:\n",
    "    if adata.shape[0] == 0 or adata.shape[1] == 0:\n",
    "        raise ValueError(f\"{name} dataset is empty: {adata.shape}\")\n",
    "    \n",
    "    # Make sure var_names and obs_names are unique\n",
    "    adata.var_names_make_unique()\n",
    "    adata.obs_names_make_unique()\n",
    "\n",
    "# Find shared genes\n",
    "shared_genes = bulk_adata.var_names.intersection(ref_adata.var_names).tolist()\n",
    "if len(shared_genes) == 0:\n",
    "    raise ValueError(\"No shared genes between bulk and reference datasets!\")\n",
    "logger.info(f\"Using {len(shared_genes)} shared genes\")\n",
    "\n",
    "# Identify cell type markers\n",
    "markers_dict = identify_cell_type_markers(\n",
    "    ref_adata, \n",
    "    annotation_key, \n",
    "    n_markers=n_markers\n",
    ")\n",
    "\n",
    "# Create signature matrix\n",
    "signature_matrix = create_signature_matrix(\n",
    "    ref_adata, \n",
    "    markers_dict, \n",
    "    annotation_key, \n",
    "    shared_genes\n",
    ")\n",
    "\n",
    "# Save signature matrix\n",
    "signature_matrix.to_csv(os.path.join(output_dir, 'signature_matrix.csv'))\n",
    "\n",
    "# Validate deconvolution approach\n",
    "validation_score = validate_deconvolution(\n",
    "    ref_adata,\n",
    "    annotation_key,\n",
    "    signature_matrix,\n",
    "    shared_genes,\n",
    "    n_samples=20\n",
    ")\n",
    "\n",
    "# Run deconvolution with confidence intervals\n",
    "deconv_results, lower_ci, upper_ci = bootstrap_confidence_intervals(\n",
    "    bulk_adata,\n",
    "    signature_matrix,\n",
    "    shared_genes,\n",
    "    n_bootstrap=n_bootstrap\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 16:47:53 - INFO - Creating deconvolution visualizations...\n",
      "2025-03-11 16:47:57 - INFO - Created deconvolution heatmap\n",
      "2025-03-11 16:47:59 - INFO - Created stacked bar chart\n",
      "2025-03-11 16:48:00 - INFO - Created example validation plot\n",
      "2025-03-11 16:48:19 - INFO - Created sample composition plots\n",
      "2025-03-11 16:48:24 - INFO - Created hierarchical clustering plot\n",
      "2025-03-11 16:48:24 - INFO - Visualization creation completed\n",
      "2025-03-11 16:48:24 - INFO - Deconvolution pipeline completed successfully\n",
      "2025-03-11 16:48:24 - INFO - Results saved to /private/groups/russelllab/jodie/wolbachia_induced_DE/wolbachia_induced_differentiation/scripts/celltype_clustering/claude/deconvolution/output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 4200x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create visualizations\n",
    "plot_deconvolution_results(deconv_results, lower_ci, upper_ci, custom_palette)\n",
    "\n",
    "# Save results\n",
    "deconv_results.to_csv(os.path.join(output_dir, 'deconvolution_results.csv'))\n",
    "lower_ci.to_csv(os.path.join(output_dir, 'deconvolution_lower_ci.csv'))\n",
    "upper_ci.to_csv(os.path.join(output_dir, 'deconvolution_upper_ci.csv'))\n",
    "\n",
    "# Create summary report\n",
    "with open(os.path.join(output_dir, 'deconvolution_summary.txt'), 'w') as f:\n",
    "    f.write(\"Cell Type Deconvolution Summary\\n\")\n",
    "    f.write(\"===============================\\n\\n\")\n",
    "    f.write(f\"Bulk dataset: {bulk_path}\\n\")\n",
    "    f.write(f\"Reference dataset: {ref_path}\\n\")\n",
    "    f.write(f\"Number of bulk samples: {bulk_adata.shape[0]}\\n\")\n",
    "    f.write(f\"Number of reference cells: {ref_adata.shape[0]}\\n\")\n",
    "    f.write(f\"Number of shared genes: {len(shared_genes)}\\n\")\n",
    "    f.write(f\"Number of cell types: {len(signature_matrix.columns)}\\n\\n\")\n",
    "    f.write(f\"Validation correlation score: {validation_score:.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Top cell types by average proportion:\\n\")\n",
    "    for cell_type, prop in deconv_results.mean().sort_values(ascending=False).items():\n",
    "        f.write(f\"  {cell_type}: {prop:.3f}\\n\")\n",
    "\n",
    "logger.info(\"Deconvolution pipeline completed successfully\")\n",
    "logger.info(f\"Results saved to {output_dir}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     logger.error(f\"Deconvolution pipeline failed: {e}\")\n",
    "#     import traceback\n",
    "#     logger.error(traceback.format_exc())\n",
    "#     return 1\n",
    "\n",
    "# return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy_ipynb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
